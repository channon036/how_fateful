---
title: "How fateful"
author: "Channon Perry"
date: "`r Sys.time()`"
output: 
   html_document:
      toc: true
      theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

packages <- c("rmarkdown", "tidyverse", "lubridate", "rjson", "ggplot2", "ggmap", "RColorBrewer")

install.packages(setdiff(packages, rownames(installed.packages())))  

library(tidyverse)
library(lubridate)
library(rjson)
library(ggplot2)
library(ggmap)
library(RColorBrewer)

```

# Introduction

When I met my boyfriend I often felt bewildered that despite living in the same city for five years and attending the same university that it took a dating app to introduce us. I'm confident that I'm not the only person to have wondered how close they came to meeting their significant other before they actually did, and thanks to Google's location history quietly ticking away collecting GPS data in the background I thought that I could potentially find a real answer to just how fatefully our paths crisscrossed before we met.

# Sourcing the data

You can request and download all of your recorded location history through the Google Maps desktop website. This data comes in a zipped folder full of JSON files. My partner kindly agreed to download his data himself, and then sent it to me.

If you're intending to use this script with your own two sets of data this article walks through the process of requesting your location history from Google in a lot of detail: <https://www.howtogeek.com/725241/how-to-download-your-google-maps-data/>

# Extracting and transforming

## Extracting

```{r extract}

#Names of two people
x <- "Chan"
y <- "Dan"

#Records file addresses (reference the Records.json file)

data_x <- "~/Crossing paths/TakeoutChannon/Location History/Records.json"
data_y <- "~/Crossing paths/TakeoutDandre/Location History/Records.json"


#Function from the fromJSON library to extract the JSON files into R list objects

extract_maps_data <- function(fileName) {
  
  records_data <- fromJSON(file = fileName)
  
  rd <- unlist(records_data, recursive = FALSE)
  
}

#Running the extract function on the data

records_data_x <- extract_maps_data(fileName = data_x)
records_data_y <- extract_maps_data(fileName = data_y)

```

## Selecting parameters + accuracy

### Distance margin

Before we jump right into transforming and cleaning the data it's necessary to set some boundaries around time frames and accuracy.

The table below displays how the accuracy measures relate to each other. 

Our data has two types of accuracy indicators. The GPS coordinates have differing number of decimal places depending on the specificity of the location. There is a also a reported accuracy measurement based on the strength of the signal of the GPS at the time and is reported in meters. 

These accuracy measures define the margin of error in distance for any of the conclusions that we draw. That is, when we potentially conclude that him and I did cross paths, did we come into tens, or hundreds, or thousands of meters from each other? I chose 11.1m, related to coordinates with 4 decimal places (+/- 00.0000), because I think that even in dense urban areas the length of a telephone pole had a tantalizingly high probability of being within sight. 

I've paired together the two types of accuracy measures so that the distance accuracy is standardised for all our insights. Coordinates with more than the stated number of decimal places are rounded. This means that we're dividing the world into bigger blocks, and moving any of the points in the middle of a block to its nearest corner. Then, any of the reported accuracy measurements with distances less than the related distance in meters are filtered out of the dataset. 


|  accuracy.level  |      1       |       2       |       3        |        4        |
|:----------------:|:------------:|:-------------:|:--------------:|:---------------:|
|  decimal places  |      3       |       4       |       5        |        6        |
|   accuracy (m)   |     111      |     11.1      |      1.11      |      0.111      |
|   coordinates    |  +/- 00.000  |  +/- 00.0000  |  +/- 00.00000  |  +/- 00.000000  |

### Time margins

We also need to define the time boundaries of the two data sets. The first date is the earliest possible date (start_looking_from) that we had any real probability of having come into contact. This could also be set to the minimum date in either of the two sets if there's no clear probable contact time like moving to the same city. 

```{r get_min_date, echo=FALSE}

min_x <- pluck(records_data_x, 1, "timestamp") %>% as.Date

print(paste(x, "'s earliest record is", min_x))

min_y <- pluck(records_data_y, 1, "timestamp") %>% as.Date

print(paste(y, "'s earliest record is", min_y))

first_intersect_date <- max(min_x, min_y)

print(paste("Use", first_intersect_date, "to start at earliest intersecting dates."))

```
In my case, fortunately my partner's records start around the time I first moved to the city.

There is also an end date to the time set. That is the date and time that we're definitely certain that our GPS coordinates matched up for the first time.

Similarly to distance margins, it's also helpful to set time intervals to group coordinates into. This is because when considering a scale of meters people just don't tend to move fast enough across the surface of the earth on a day-to-day basis for it to be necessary to record our location many times a second, however, the data is stored down to millisecond time stamps. 

I chose 5 minutes because it was also a margin I was happy with having on either side of my insights, i.e. we may have sat on the same bench 5 minutes apart.  

```{r parameters_accuracy}

#Distance margin
accuracy_level <- 1

#Time margins
time_accuracy <- "30 mins" #The granularity of time buckets. Use ?round_date to see time categories available.

start_looking_from <- as.POSIXct(first_intersect_date) #Start date to start analysing data from
  
best_day <- as.POSIXct("2020/08/08 1:00:00") #The confirmed first meeting <3

```

```{r parameters background, echo = FALSE}

#This ties together the two types of accuracy filters into one choice by setting the number of decimals in a coordinate to the same accuracy or more than the reported accuracy so that all results are guaranteed to have the minimum accuracy in the table above. 

ra <- c(111, 11.1, 1.11, 0.111) #recorded accuracy
dp <- c(3, 4, 5, 6) #decimal places
ac <- c(ra, dp)

accuracy <- matrix(ac, nrow = 2, byrow = TRUE)

distance_accuracy <- accuracy[2, accuracy_level] 

google_reported_accuracy <- accuracy[1, accuracy_level]

```

## Transforming the data

The tree-style lists I've converted the JSON files from are still in an unrefined state. Currently, each data set is made up of hundreds of thousands of observations, each of which is in a listed format with the below structure:

```{r exlist, echo = FALSE}
str(records_data_x[[1]], max.level = 1)
```

By the time we're complete transforming the data we would like to have two data frames, one for each person, with the following structure:

|  index  |  timestamp   | latitude  | longitude | accuracy | person | coordinates |
|:-------:|:------------:|:---------:|:---------:|:--------:|:------:|:-----------:|
| 1:nrows | POSIXct time | dd coords | dd coords | m radius |  name  |  lat, long  |

To do that we'll run the following function on both data sets:

```{r transform_function}

transform_maps_data <- function(x, person, time_accuracy){
  
  # Extracting the variables that we're interested in and dropping all the information related to whether Google thinks we were in a vehicle or sitting still. 
  
  timestamp <- lapply(x, pluck, "timestamp") %>% unlist() %>% data.frame()
  latitude <- lapply(x, pluck, "latitudeE7") %>% unlist() %>% data.frame()
  longitude <- lapply(x, pluck, "longitudeE7") %>% unlist() %>% data.frame()
  accuracy <- lapply(x, pluck, "accuracy") %>% unlist() %>% data.frame()
  
  # Setting an index 
  index <- 1:nrow(timestamp)
  
  # Combining each of the variables into a data frame as columns with clear names.
  
  records_df <- cbind(timestamp, latitude, longitude, accuracy)
  rownames(records_df) <- index
  colnames(records_df) <- c("timestamp", "latitude", "longitude", "accuracy")
  
  # Adjusting the formatting and adding some key columns. 
  output <- records_df %>%
    
  # Formatting the time stamps so that they are POSIXct objects in the system timezone and rounding them off into the given accuracy limits
    mutate(timestamp_utc = as.POSIXct(timestamp, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")) %>%
    mutate(timestamp = with_tz(timestamp_utc, tzone = Sys.timezone())) %>%
    select(-timestamp_utc) %>%
    mutate(timestamp = round_date(timestamp, unit = time_accuracy)) %>% 
    
  # Adding a column with the name of the individual who the records belong to
    mutate(person = person) %>%
    
  # Latitude and longitude are in a suitable format to allow for quick copy and pasting into Google Maps, and are filtered within the distance.accuracy number of decimal points
    
    mutate(latitude = round((latitude/10000000), distance_accuracy)) %>%
    mutate(longitude = round((longitude/10000000), distance_accuracy)) %>%
    
  # Adding a column for combined coordinates
    mutate(coordinates = paste(latitude, longitude, sep = ", ")) %>% 
    
  # Filtering our any records outside of reported accuracy
    filter(accuracy <= google_reported_accuracy) %>%
    filter(accuracy >= 0) %>%
    
  # Filtering our any records before or after start and end dates
    filter(timestamp >= start_looking_from) %>%
    filter(timestamp <= best_day) %>%
    
  # Removing duplicates
    unique()
  
}

```

```{r transform}

x_records <- transform_maps_data(records_data_x, person = x, time_accuracy = time_accuracy)

y_records <- transform_maps_data(records_data_y, person = y, time_accuracy = time_accuracy)

```

At the end of our transform step we have two data frames with the following structure:

```{r exdf, echo = FALSE}
head(x_records)
```

# Save

Saving the files

```{r load}

save(x_records, file = "X_Google_Maps_history.RData")
save(y_records, file = "Y_Google_Maps_history.RData")

```

# Analysis

### Combining the two datasets together

Next I'll combine the two sets of records together to get a long version of all the records.

```{r combined_records, echo=FALSE}


columns <- c("person", "timestamp", "coordinates", "latitude", "longitude", "accuracy")

combined <- full_join(x_records, y_records, by = columns) %>% 
  group_by(timestamp, coordinates, person) %>%
  mutate(max_accuracy = min(accuracy), .keep = "unused") %>%
  unique()

arrange(combined, coordinates) %>% head(5)


```

### Places we've both frequently visited

This is an initial view of some of the most common places we both visited.

```{r place_groups, echo = FALSE, message = FALSE}

#Group the data together by places
places <- combined %>%
  group_by(coordinates, person) %>%
  summarise(latitude, longitude, timestamp, records = n()) %>%
  arrange(desc(records)) 

#Find all the places x has been
x_places <- places %>%
  filter(person == x)

#Find all places y has been
y_places <- places %>%
  filter(person == y)

#Look for the intersection of places that both x and y had been
both_places <- full_join(x_places, y_places, by = c("latitude","longitude", "coordinates")) %>%
  mutate(both_records = records.x + records.y) %>%
  mutate(x_per = (records.x/both_records)*100) %>%
  mutate(y_per = (records.y/both_records)*100) %>%
  mutate(dif_per = x_per - y_per) %>%
  mutate(time_dif = abs(difftime(timestamp.x, timestamp.y, units = "auto"))) %>%
  filter(!is.na(records.x)) %>%
  filter(!is.na(records.y)) %>%
  unique() %>%
  arrange(desc(both_records)) 

```

#### Visualizing all the places we had both been

```{r map_common_locations, warning = FALSE, message=FALSE}

# Get Map data from Google API

# Note: you'll need to get a key on the Google Maps platform to access the API.

# Go to https://mapsplatform.google.com/ -> get started -> credentials -> API keys 
# Find Maps API Key and SHOW KEY, then copy and paste your API key below:


#Set a colour palette 
my_palette <- colorRampPalette(brewer.pal(3, "Spectral"))

#plot the map, scaling by number of records, and colouring by who visited most frequently
ggmap(map_most_common_places, darken = c(0.30, "black")) +
  geom_point(aes(x = longitude, y = latitude, size = both.records, colour = dif_per),
             data = most_places, 
             alpha = 0.8)+
  scale_radius(range = c(1,10), 
             name = "Number of records") +
  scale_colour_gradientn(colours = my_palette(3), 
                         name = "Most visited by person", 
                         breaks = c(min(most_places$dif_per), 0, max(most_places$dif_per)),
                         labels = c(y, "both", x)) +
  theme(text=element_text(family="sans"),
        axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank()) +
  ggtitle(label = "Common locations")

```

There were `r n_distinct(both_places$coordinates)` coordinates we were both recorded in at some point during the five year time frame. It's clear that most of the common locations are the main roads in and around the city. However, interestingly my partner had been within `r accuracy[2,accuracy.level]` of both my office and my apartment. 

### How close in time did we get to meeting in any of those places?

Now that I've been able to whittle down the potential locations we could have meet, next we'll need to measure how near in time we came to being at those locations.

We've already calculated the time difference (time_diff) between any two visits in the both_places data frame. This is because when joining the two data frames for each timestamp of person x's visits in a set of coordinates, there is corresponding data for each timestamp of person y's visits to a particualr set of coordinates.

```{r minimum_time_differences, warning = FALSE, message = FALSE}

#Group by coordinate and then include the instances with the minimum time differences between visits

grouped_places <- both_places %>% 
  group_by(coordinates) %>%
  mutate(closest_visit = min(time_dif)) 

closest_timestamps <- grouped_places %>%
  unique() %>%
  filter(time_dif == closest_visit) %>%
  mutate(closest_visit = as.numeric(as.duration(closest_visit)))

map_close_brushes <- get_map(
  location = c(lon = median(closest_timestamps$longitude), lat = median(closest_timestamps$latitude)), 
  zoom = 14,
  size = c(640, 640),
  maptype = "toner-lite",
  color = "bw")

ggmap(map_close_brushes, 
      darken = c(0.30, "black")) +
  geom_point(aes(x = longitude, y = latitude, color = closest_visit),
             data = closest_timestamps, 
             size = 2,
             alpha = 0.8) +
  theme(text=element_text(family="sans"),
        axis.title=element_blank(),
        axis.text=element_blank(),
        axis.ticks = element_blank(),
        panel.grid = element_blank()) +
  ggtitle(label = "Close encounters") +
   geom_text(data = closest_timestamps,
            aes(longitude, latitude, label = closest_visit),
            colour = "white", 
            check_overlap = TRUE,
            size = 3, 
            hjust = 0.5, 
            vjust = -1.2)


```

### How close did we ever get to each other at any given time?

So it seems like when grouping by locations with a maximum of a 11m radius we were always hundreds of hours apart from each other. This makes it seem like we never even got close to meeting. Another way to interrogate the data is then to ask, how close did we ever get to each other at any given time?

```{r all_time_intervals, message=FALSE, warning=FALSE}

all_intervals <- seq(from = start_looking_from, to = best_day, by = "min") %>%
  as.data.frame()
colnames(all_intervals) <- "timestamp"

whole_time_x <- full_join(all_intervals, x_records, by = "timestamp") %>% 
  mutate(person = x) 

whole_time_y <- full_join(all_intervals, y_records, by = "timestamp") %>% 
  mutate(person = y) 

#I'm using the haversine formula to calculate the distance between two points on earth's spherical surface. This is because the harversine formula offers the most accuracy on the scale of meters.

haversine_calc <- function(lat.x, lat.y, lon.x, lon.y){
  
  #radius of the earth in km
  r <- 6371
  
  #converting degrees to radians
  r.lat.x <- (lat.x*pi)/180
  r.lat.y <- (lat.y*pi)/180
  r.lon.x <- (lon.x*pi)/180
  r.lon.y <- (lon.y*pi)/180
  
  dlat <- r.lat.y - r.lat.x
  dlon <- r.lon.y - r.lon.x
  
  a <- sin(dlat/2)**2 + cos(r.lat.x) * cos(r.lat.y) * sin(dlon/2)**2
  
  c <- 2 * asin(sqrt(a))
  
  d <- c * r
  
}

whole_time_both <- full_join(whole_time_x, whole_time_y, by = "timestamp") %>%
  filter(!is.na(coordinates.x)) %>%
  filter(!is.na(coordinates.y)) %>%
  mutate(distance = haversine_calc(lat.x = latitude.x,
                                   lat.y = latitude.y,
                                   lon.x = longitude.x,
                                   lon.y = longitude.y)*1000) %>%
  arrange(distance)

#movable-type.co.uk/scripts/latlong.html
#https://www.geeksforgeeks.org/program-distance-two-points-earth/#:~:text=For%20this%20divide%20the%20values,is%20the%20radius%20of%20Earth.




```

So the closest we ever really got was `r min(whole_time_both$distance)` meters to each other. I'd say that wasn't even close. 

Let's look at this over time.

```{r the_distance_over_time}

whole_time_both$accuracy <- whole_time_both$accuracy.x + whole_time_both$accuracy.y

ggplot(whole_time_both, aes(x = timestamp, y = distance)) +
  geom_point() +
  ylim(0, 1000) +
  geom_errorbar(aes(ymin = distance-accuracy, ymax = distance+accuracy))

```



Let's take a look at the closest pairs of coordinates at any given time. 

### Visualising how close we got

```{r the_closest_distances, message = FALSE}

less_than_500m <- whole_time_both %>% filter(distance <= 500)

map_close_distances <- get_map(
  location = c(
    lon = median(c(less_than_500m$longitude.x, less_than_500m$longitude.y)), 
    lat = median(c(less_than_500m$latitude.x, less_than_500m$latitude.y))
    ), 
  zoom = 17,
  size = c(640, 640),
  maptype = "toner-lite",
  color = "bw")

ggmap(map_close_distances, 
      darken = c(0.30, "black")) +
  geom_point(aes(x = longitude.x, y = latitude.x),
             data = less_than_500m, 
             colour = two_colours[1],
             size = 1,
             alpha = 0.3, 
             position = "jitter") +
  geom_point(aes(x = longitude.y, y = latitude.y),
             data = less_than_500m, 
             colour = two_colours[2],
             size = 1,
             alpha = 0.3, 
             position = "jitter") +
  theme_classic()+
  ggtitle(label = "Crossing paths on campus") 
# +
  #  geom_text(data = less_than_500m,
  #           aes(longitude.x, latitude.x, label = format(timestamp, format = "%H:%M")),
  #           colour = "white", 
  #           check_overlap = TRUE,
  #           size = 4, 
  #           hjust = 1.5, 
  #           vjust = 0.15) +
  # geom_text(data = less_than_500m,
  #           aes(longitude.y, latitude.y, label = format(timestamp, format = "%H:%M")),
  #           colour = "white", 
  #           check_overlap = TRUE,
  #           size = 4, 
  #           hjust = 1.5, 
  #           vjust = 0.15)

```

On `r as.Date(min(less_than_500m$timestamp))` we were recorded being less than 500m from one another. On this day, it appears that we both move towards the bus stop at the north of the campus around lunch time. Could we have taken the bus together? The truth is, it seems like despite all the conditions theoretically allowing us the opportunity to run into each other, the one thing that must have kept us apart was that it just wasn't the right time yet.

NB ==================================================================================
X vs Y
time vs distance