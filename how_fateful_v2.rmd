---
title: "How fateful?"
author: "Channon Perry"
date: "`r Sys.Date()`"
output: 
   html_document:
     theme: cosmo
     toc: TRUE
     toc_float: TRUE
---

```{css, echo=FALSE, message = FALSE}

knitr::opts_chunk$set(echo = TRUE)

.code {
  background-color: #FFF8C7;
  border: 3px #38315F;
  
knitr::opts_chunk$set(class.source = "code")
knitr::opts_chunk$set(fig.width=16, fig.height=8) 

```

# Introduction

When I met my boyfriend I sometimes felt a tinge of regret that despite living in the same city for five years and attending the same university that we never got to make an adorable meet-cute story. Our story isn't unique to many couples in 2020- in the trough between COVID waves we started speaking online. On Bumble, a dating app, to be specific.

Thanks to Google's location history quietly ticking away collecting GPS data in the background I thought that I could potentially find a real answer to how fatefully our paths crisscrossed before we met. Just how closely did we come to potentially having an eyes-meet-across-the-room moment?

*Scroll down to 'The Answer' on the table of contents on the left if you're not interested in the technical stuff!*

# Sourcing the data

You can request and download all of your recorded location history through the Google Maps desktop website. This data comes in a zipped folder full of JSON files. My partner kindly agreed to download his data himself, and then sent it to me.

```{r setup, message = FALSE, warning = FALSE}

packages <-
  c("tidyverse",
    "lubridate",
    "rjson",
    "ggplot2",
    "ggmap",
    "wesanderson")

install.packages(setdiff(packages, rownames(installed.packages())))

library(tidyverse)
library(lubridate)
library(rjson)
library(ggplot2)
library(ggmap)
library(wesanderson)

```

## Extract

```{r extract}

#Names of two people
x <- "Chan"
y <- "Dan"

#Records file addresses (reference the Records.json file)

data_x <-
  "~/Crossing paths/TakeoutChannon/Location History/Records.json"
data_y <-
  "~/Crossing paths/TakeoutDandre/Location History/Records.json"

#Function from the fromJSON library to extract the JSON files into R list objects

extract_maps_data <- function(fileName, saveAs) {
  rd <- fromJSON(file = fileName)
  
  records_data <- unlist(rd, recursive = FALSE)
  
  save(records_data, file = saveAs)
  
  records_data
  
}

#Running the extract function on the data if it hasn't been run before, otherwise loading it in

if (file.exists("extracted_data_x.RData")) {
  records_data_x <- get(load("extracted_data_x.RData"))
} else {
  records_data_x <- extract_maps_data(fileName = data_x,
                                      saveAs = "extracted_data_x.RData")
}

if (file.exists("extracted_data_y.RData")) {
  records_data_y <- get(load("extracted_data_y.RData"))
} else {
  records_data_y <- extract_maps_data(fileName = data_y,
                                      saveAs = "extracted_data_y.RData")
}

```

## Transform

### Set parameters

Before we jump right into transforming and cleaning the data it's necessary to set some boundaries around time frames and accuracy so that we don't spend time processing unnecessary records.

#### Distance margin

This table displays how the distance accuracy measures relate to each other.

| accuracy.level |     1      |      2      |      3       |       4       |
|:--------------:|:----------:|:-----------:|:------------:|:-------------:|
| decimal places |     3      |      4      |      5       |       6       |
|  accuracy (m)  |    111     |    11.1     |     1.11     |     0.111     |
|  coordinates   | +/- 00.000 | +/- 00.0000 | +/- 00.00000 | +/- 00.000000 |

Our data has two types of accuracy indicators. The GPS coordinates have differing number of decimal places depending on the specificity of the location. There is a also a reported accuracy measurement based on the strength of the signal of the GPS at the time and is reported in meters.

These accuracy measures define the margin of error in distance for any of the conclusions that we draw. That is, when we potentially conclude that him and I did cross paths, did we come into tens, or hundreds, or thousands of meters from each other? I initially chose 11.1m, related to coordinates with 4 decimal places (+/- 00.0000), because I thought that even in dense urban areas the length of a telephone pole had a tantalizingly high probability of being within sight. However, the quality of the data (see below) limited the records too much, forcing me to use a distance accuracy of 111m instead.

I've paired together the two types of accuracy measures so that the distance accuracy is standardised for all our insights. Coordinates with more than the stated number of decimal places are rounded. This means that we're dividing the world into bigger blocks, and moving any of the points in the middle of a block to its nearest corner. Then, any of the reported accuracy measurements with distances less than the related distance in meters are filtered out of the data set.

```{r distance_accuracy}

#Distance margin
accuracy_level <- 1

```

#### Time margins

We also need to define the time boundaries of the two data sets. The first date is the earliest possible date that we had any real probability of having come into contact. This could also be set to the minimum date in either of the two sets if there's no clear probable contact time like moving to the same city.

```{r get_min_date, echo=FALSE}

min_x <- pluck(records_data_x, 1, "timestamp") %>% as.Date

paste(x, "'s earliest record is", min_x)

min_y <- pluck(records_data_y, 1, "timestamp") %>% as.Date

paste(y, "'s earliest record is", min_y)

first_intersect_date <- max(min_x, min_y)

paste(
  "Use",
  first_intersect_date,
  "or first_intersect_date to start at earliest intersecting dates."
)

```

```{r start_date}

start_looking_from <- as.POSIXct(first_intersect_date) #Start date to start analysing data from

```

There is also an end date to the time set. That is the date and time that we're definitely certain that our GPS coordinates would have matched up for the first time.

```{r first_match}

best_day <- as.POSIXct("2020/08/08 1:00:00") #The confirmed first meeting <3

```

Similarly to distance margins, it's also helpful to set time intervals to group coordinates into. This is because when considering a scale of meters people just don't tend to move fast enough across the surface of the earth on a day-to-day basis for it to be necessary to record our location many times a second, however, the data is stored down to millisecond time stamps.

I chose 10 minutes because it was also a margin I was happy with having on either side of my insights, i.e. we may have sat on the same bench 10 minutes apart.

```{r parameters_accuracy}

#Time margins
time_accuracy <- "10 mins" #The granularity of time buckets. Use ?round_date to see time categories available.

```

```{r background_table, echo = FALSE}


#This ties together the two types of accuracy filters into one choice by setting the number of decimals in a coordinate to the same accuracy or more than the reported accuracy so that all results are guaranteed to have the minimum accuracy in the table above.

ra <- c(111, 11.1, 1.11, 0.111) #recorded accuracy
dp <- c(3, 4, 5, 6) #decimal places
ac <- c(ra, dp)

accuracy <- matrix(ac, nrow = 2, byrow = TRUE)

distance_accuracy <- accuracy[2, accuracy_level]

google_reported_accuracy <- accuracy[1, accuracy_level]

```

The tree-style lists I've converted the JSON files from are still in an unrefined state. Currently, each data set is made up of hundreds of thousands of observations, each of which is in a listed format with the below structure:

```{r exlist, echo = FALSE}
str(records_data_x[[1]], max.level = 1)
```

By the time we're complete transforming the data we would like to have two data frames, one for each person, with the following structure:

|  index  |  timestamp   | latitude  | longitude | accuracy | person | coordinates |    source     |
|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|
| 1:nrows | POSIXct time | dd coords | dd coords | m radius |  name  |  lat, long  | signal source |

To do that we'll run the following function on both data sets:

```{r transform_function}

transform_maps_data <-
  function(x,
           person,
           time_accuracy,
           distance_accuracy) {
    # Extracting the variables that we're interested in and dropping all the information related to whether Google thinks we were in a vehicle or sitting still.
    
    timestamp <-
      lapply(x, pluck, "timestamp") %>% unlist() %>% data.frame()
    latitude <-
      lapply(x, pluck, "latitudeE7") %>% unlist() %>% data.frame()
    longitude <-
      lapply(x, pluck, "longitudeE7") %>% unlist() %>% data.frame()
    accuracy <-
      lapply(x, pluck, "accuracy") %>% unlist() %>% data.frame()
    source <- lapply(x, pluck, "source") %>% unlist() %>% data.frame()
    
    # Setting an index
    index <- 1:nrow(timestamp)
    
    # Combining each of the variables into a data frame as columns with clear names.
    
    records_df <-
      cbind(timestamp, latitude, longitude, accuracy, source)
    rownames(records_df) <- index
    colnames(records_df) <-
      c("timestamp", "latitude", "longitude", "accuracy", "source")
    
    # Adjusting the formatting and adding some key columns.
    output <- records_df %>%
      
      # Formatting the time stamps so that they are POSIXct objects in the system timezone and rounding them off into the given accuracy limits
      mutate(timestamp_utc = as.POSIXct(timestamp, format = "%Y-%m-%dT%H:%M:%S", tz = "UTC")) %>%
      mutate(timestamp = with_tz(timestamp_utc, tzone = Sys.timezone())) %>%
      select(-timestamp_utc) %>%
      mutate(timestamp = round_date(timestamp, unit = time_accuracy)) %>%
      
      # Adding a column with the name of the individual who the records belong to
      mutate(person = person) %>%
      
      # Latitude and longitude are in a suitable format to allow for quick copy and pasting into Google Maps, and are filtered within the distance.accuracy number of decimal points
      
      mutate(latitude = round((latitude / 10000000), distance_accuracy)) %>%
      mutate(longitude = round((longitude / 10000000), distance_accuracy)) %>%
      
      # Adding a column for combined coordinates
      mutate(coordinates = paste(latitude, longitude, sep = ", ")) %>%
      
      # Filtering our any records outside of reported accuracy
      filter(accuracy <= google_reported_accuracy) %>%
      filter(accuracy >= 0) %>%
      
      # Filtering our any records before or after start and end dates
      filter(timestamp >= start_looking_from) %>%
      filter(timestamp <= best_day) %>%
      
      # Removing unknown sources for integrity
      filter(source != "UNKNOWN") %>%
      
      # Removing duplicates
      unique()
    
  }

#Running the function on the two data sets or otherwise loading data we've already saved before.

if (file.exists("X_Google_Maps_history.RData")) {
  load("X_Google_Maps_history.RData")
} else{
  x_records <-
    transform_maps_data(
      records_data_x,
      person = x,
      time_accuracy = time_accuracy,
      distance_accuracy = distance_accuracy
    )
}

if (file.exists("Y_Google_Maps_history.RData")) {
  load("Y_Google_Maps_history.RData")
} else{
  y_records <-
    transform_maps_data(
      records_data_y,
      person = y,
      time_accuracy = time_accuracy,
      distance_accuracy = distance_accuracy
    )
}



```

At the end of our transform step we have two data frames with the following structure:

```{r exdf, echo = FALSE}
head(x_records)
```

## Load

Saving the files.

```{r save}

save(x_records, file = "X_Google_Maps_history.RData")
save(y_records, file = "Y_Google_Maps_history.RData")

```

# Analysis

## Combining the records

Working with one dataset is easier than working with two in this case so we'll create one long-version of all the records for analysis.

```{r combined_records, echo=FALSE}


if(file.exists("combined_records.RData")) {
  get(load("combined_records.RData"))
  
} else{
  columns <-
    c(
      "person",
      "timestamp",
      "coordinates",
      "latitude",
      "longitude",
      "accuracy",
      "source"
    )
  
  combined <- full_join(x_records, y_records, by = columns) %>%
    group_by(timestamp, coordinates, person) %>%
    mutate(max_accuracy = min(accuracy), .keep = "unused") %>%
    unique()
  
}

save(combined, file = "combined_records.RData")

arrange(combined, coordinates) %>% head(5)


```

```{r colour_schemeing, echo = FALSE}

two_colours <- wes_palette("Darjeeling1", n = 2)

three_colours <-  wes_palette("Darjeeling1", n = 3)

```

## Data quality and completeness {.tabset}

### Data completeness

```{r data_quality_assesment, echo = FALSE}

# How much of our time was recorded by Google Maps?

total_time <-
  difftime(best_day, start_looking_from, units = "mins") %>%
  as.numeric() / 10

recorded_time <- combined %>%
  group_by(person) %>%
  summarise(n = n()) %>%
  mutate(per = n / total_time)

x_per_records <-
  glue::glue(as.numeric(round(recorded_time[1, 3], 2)) * 100, "%")

xs <- glue::glue(x, "'s")
ys <- glue::glue(y, "'s")

y_per_records <-
  glue::glue(as.numeric(round(recorded_time[2, 3], 2)) * 100, "%")

records_avaliable <-
  ggplot(combined, aes(x = timestamp, fill = person)) +
  geom_histogram(
    position = "identity",
    bins = round(total_time / 1440),
    alpha = 0.5
  ) +
  scale_fill_manual(values = two_colours) +
  theme_classic() +
  labs(y = "Number of records",
       x = "Date",
       title = "Avaliability of records")

```

An analysis of how many records are available to who and for which time periods show that `r x_per_records` of `r xs` time periods had records that meet the accuracy standards. In comparison, `r y_per_records` of `r ys` time periods contained records.

Gaps in records could be due to a lack of mobile data (which is notably expensive in South Africa) or our cellphones being turned off while traveling. Further, there may be gaps when privacy settings disallowed the collection of location history.

These variables means that no matter the result, there will still be some uncertainty during the un-recorded periods where we may have crossed pathways but no evidence exists.

```{r records_avaliable_chart, fig.cap= "The timeline under review, showing the avaliability of records per person. Only time periods with records for both people will contain potential matches", echo = FALSE}

records_avaliable


```

### Source composition of records

```{r source_composition_of_records, echo = FALSE, message = FALSE}

source_record_composition <-
  ggplot(combined, aes(x = source, fill = person)) +
  geom_bar(position = "dodge") +
  labs(x = "Source",
       y = "Number of records",
       title = "Where did most of the records source from?") +
  scale_fill_manual(values = two_colours) +
  theme_classic()

unique_location_sources <- combined %>%
  group_by(source, coordinates) %>%
  summarise() %>%
  group_by(source) %>%
  unique() %>%
  summarise(unique_locations = n()) %>%
  arrange(desc(unique_locations))

count_gps_coords <- unique_location_sources[1, 2]
count_wifi_coords <- unique_location_sources[2, 2]
count_cell_coords <- unique_location_sources[3, 2]

unique_records_sources <- combined %>%
  select(source, timestamp) %>%
  group_by(source) %>%
  unique() %>%
  summarise(unique_records = n()) %>%
  arrange(desc(unique_records))

count_gps_records <- unique_records_sources[1, 2]
count_wifi_records <- unique_records_sources[2, 2]
count_cell_records <- unique_records_sources[3, 2]

composition_table <-
  merge(unique_location_sources, unique_records_sources, by = "source") %>%
  mutate(records_per_location = round(unique_records / unique_locations, 1))

```

Analysing the source of the records show that the majority of the collected records came from instances where the cellphone was connected to WiFi.

```{r source_composition_of_records_chart, fig.cap= "The number of records by person and by source.", echo = FALSE}

source_record_composition

```

However, when considering the number of unique coordinates per source it's evident that most of the unique locations came from GPS, and that WiFi and cell records where tied to a handful of locations. Only `r count_wifi_coords` WiFi routers made up all of the `r count_wifi_records` records. There was also only `r count_cell_coords` cell phone towers with unique coordinates making up `r count_cell_records` records.

We can understand this better by looking to the aggregate number of unique records per unique locations by source. WiFi had the greatest number of records per location because everything in range of the WiFi router was centralised to the router's location when cellphones connected to WiFi. The same is true for cell towers, however at a lesser rate. GPS records were the most likely to be detailed.

```{r tables_locations_per_source, echo = FALSE}

composition_table


```

### Source accuracy

```{r accuracy_by_source, echo = FALSE}

get_box_stats <-
  function(y,
           upper_limit = max(combined$max_accuracy) * 1.15) {
    return(data.frame(
      y = 0.95 * upper_limit,
      label = paste("Count =", length(y), "\n",
                    "Average =", round(mean(y), 2), "\n")
    ))
  }

source_accuracy <-
  ggplot(combined, aes(x = source, y = max_accuracy)) +
  geom_boxplot() +
  labs(x = "Source of record",
       y = "Distance accuracy (m)",
       title = "Average accuracy of sources") +
  theme_classic() +
  stat_summary(
    fun.data = get_box_stats,
    geom = "text",
    hjust = 0.5,
    vjust = 0.4
  )

```

Finally, it's important to consider the differences in accuracy of the different sources. Along with being the most likely to record coordinates away from centralised WiFi routers or cell towers, GPS records are also on average the most accurate.

```{r accuracy_by_source_chart, fig.cap= "Boxplot showing accuracy statistics by record source. Remember that the minimum accuracy is set above, so the graph only considers records within the stated parameters.", echo = FALSE}

source_accuracy

```

## Places we've both frequently visited

This is an initial view of some of the most common places we both visited.

```{r place_groups, message = FALSE}

#Group the data together by places
places <- combined %>%
  group_by(coordinates, person) %>%
  summarise(latitude, longitude, timestamp, source, records = n()) %>%
  arrange(desc(records))

#Find all the places x has been
x_places <- places %>%
  filter(person == x)

#Find all places y has been
y_places <- places %>%
  filter(person == y)

#Look for the intersection of places that both x and y had been

if (file.exists("both_places.RData")) {
  both_places <- get(load("both_places.RData"))
  
} else {
  both_places <-
    full_join(x_places,
              y_places,
              by = c("latitude", "longitude", "coordinates", "source")) %>%
    filter(!is.na(records.x)) %>%
    filter(!is.na(records.y)) %>%
    mutate(both_records = records.x + records.y) %>%
    mutate(x_per = (records.x / both_records) * 100) %>%
    mutate(y_per = (records.y / both_records) * 100) %>%
    mutate(dif_per = x_per - y_per) %>%
    # mutate(time_dif = abs(difftime(timestamp.x, timestamp.y, units = "auto"))) %>%
    mutate(time_dif = abs(timestamp.x - timestamp.y)) %>%
    unique() %>%
    arrange(desc(both_records))
  
}

save(both_places, file = "both_places.RData")

```

#### Visualizing all the places we had both been

```{r map_common_locations, warning = FALSE, message=FALSE}

# Get Map data from Google API

# Note: you'll need to get a key on the Google Maps platform to access the API.

# Go to https://mapsplatform.google.com/ -> get started -> credentials -> API keys
# Find Maps API Key and SHOW KEY, then copy and paste your API key below:
# register_google(key = "[your key]", write = TRUE)

#Set a colour palette
my_palette <- c("gold", "green3", "royalblue3")

#plot the map, scaling by number of records, and colouring by who visited most frequently

map_most_common_places <- get_map(
  location = c(
    lon = median(both_places$longitude),
    lat = median(both_places$latitude)
  ),
  zoom = 12,
  scale = "auto",
  maptype = "toner-lite"
)

common_places_map <- ggmap(map_most_common_places) +
  geom_point(
    aes(
      x = longitude,
      y = latitude,
      size = both_records,
      colour = dif_per
    ),
    data = both_places,
    alpha = 0.8,
    position = "jitter"
  ) +
  scale_radius(range = c(1, 10),
               name = "Number of records") +
  scale_colour_gradientn(
    colours = three_colours,
    name = "Most visited by person",
    breaks = c(min(both_places$dif_per), 0, max(both_places$dif_per)),
    labels = c(y, "both", x)
  ) +
  theme(
    text = element_text(family = "sans"),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  ) +
  ggtitle(label = "Common locations")

common_places_map

```

There were `r n_distinct(both_places$coordinates)` coordinates we were both recorded in at some point during the five year time frame. It's clear that most of the common locations are the main roads in and around the city. I lived and worked in the northern part of the city, and that's visible by all the yellow. My partner spent most of his time living and studying around the university.

## How close in time did we get to meeting in any of those places?

Now that I've been able to whittle down the potential locations we could have meet, next we'll need to measure how near in time we came to being at those locations.

We've already calculated the time difference (time_diff) between any two visits in the both_places data frame. This is because when joining the two data frames for each time-stamp of person x's visits in a set of coordinates, there is corresponding data for each time-stamp of person y's visits to a particular set of coordinates.

```{r minimum_time_differences, warning = FALSE, message = FALSE}

#To minimise the time it takes to run we can begin by filtering out the values with time differences of more than 12 hours (the vast majority of the records).

both_places_under_12_hours <- both_places %>%
  filter(time_dif < (12 * 60 * 60))

#Then we can group by coordinate and include the instances with the minimum time differences between visits per coordinate

grouped_places <- both_places_under_12_hours %>%
  group_by(coordinates) %>%
  mutate(closest_visit = min(time_dif))

closest_timestamps <- grouped_places %>%
  unique() %>%
  filter(time_dif == closest_visit) %>%
  mutate(closest_visit = as.numeric(closest_visit) / 60)

crossed_paths <- closest_timestamps %>% filter(time_dif == 0)

crossed_paths_n <- crossed_paths %>% length()

map_close_brushes <- get_map(
  location = c(
    lon = median(closest_timestamps$longitude),
    lat = median(closest_timestamps$latitude)
  ),
  zoom = 15,
  scale = "auto",
  maptype = "toner-lite"
)

close_brushes_map <- ggmap(map_close_brushes,
                           darken = c(0.30, "black")) +
  geom_point(
    aes(x = longitude, y = latitude, color = closest_visit),
    data = closest_timestamps,
    size = 2,
    alpha = 0.7,
    position = "jitter"
  ) +
  scale_colour_gradientn(colours = three_colours, name = "Minutes  apart (+/- 10 mins)") +
  theme(
    text = element_text(family = "sans"),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  ) +
  ggtitle(label = "Close encounters") +
  geom_text(
    data = closest_timestamps,
    aes(longitude, latitude, label = closest_visit),
    colour = "white",
    check_overlap = TRUE,
    size = 3,
    hjust = 0.5,
    vjust = -1.2
  )
close_brushes_map



```

Oh how exciting! There were `r crossed_paths_n` instances (largely around campus) where we had come closer than `r accuracy[1,accuracy_level]` meters from another `r time_accuracy` or less apart.

### How close did we ever get to each other at any given time?

So when did these close brushes occur? Let's plot them over time.

```{r all_time_intervals, message=FALSE, warning=FALSE}

all_intervals <-
  seq(from = start_looking_from, to = best_day, by = "10 min") %>%
  as.data.frame()
colnames(all_intervals) <- "timestamp"

whole_time_x <-
  full_join(all_intervals, x_records, by = "timestamp") %>%
  mutate(person = x)

whole_time_y <-
  full_join(all_intervals, y_records, by = "timestamp") %>%
  mutate(person = y)

#I'm using the haversine formula to calculate the distance between two points on earth's spherical surface. This is because the harversine formula offers the most accuracy on the scale of meters.

haversine_calc <- function(lat.x, lat.y, lon.x, lon.y) {
  #radius of the earth in km
  r <- 6371
  
  #converting degrees to radians
  r.lat.x <- (lat.x * pi) / 180
  r.lat.y <- (lat.y * pi) / 180
  r.lon.x <- (lon.x * pi) / 180
  r.lon.y <- (lon.y * pi) / 180
  
  dlat <- r.lat.y - r.lat.x
  dlon <- r.lon.y - r.lon.x
  
  a <- sin(dlat / 2) ** 2 + cos(r.lat.x) * cos(r.lat.y) * sin(dlon / 2) **
    2
  
  c <- 2 * asin(sqrt(a))
  
  d <- c * r
  
}

whole_time_both <-
  full_join(whole_time_x, whole_time_y, by = "timestamp") %>%
  filter(!is.na(coordinates.x)) %>%
  filter(!is.na(coordinates.y)) %>%
  mutate(
    distance = haversine_calc(
      lat.x = latitude.x,
      lat.y = latitude.y,
      lon.x = longitude.x,
      lon.y = longitude.y
    ) * 1000
  ) %>%
  arrange(distance)

whole_time_both$accuracy <-
  whole_time_both$accuracy.x + whole_time_both$accuracy.y

whole_time_simple <- whole_time_both %>%
  select(timestamp, distance, accuracy) %>%
  mutate(day = as.Date(timestamp)) %>%
  group_by(day) %>%
  transmute(min_dist = min(distance), min_acc = min(accuracy)) %>%
  mutate(closest_brush = (min_dist == 0)) %>%
  unique()

ggplot(whole_time_simple,
       aes(x = day, y = min_dist, color = closest_brush)) +
  geom_point(
    alpha = 0.5,
    position = "jitter",
    fill = "white",
    size = 3
  ) +
  scale_color_discrete(two_colours, name = "Closest brush?") +
  theme_classic() +
  labs(x = "Date",
       y = "Distance from each other (meters)") +
  ylim(0, 2500)


```

Between 2016-2018 while I was studying and before I graduated the distances between us on any given day tended to be closer than from 2018 when I stopped going to campus. This makes me think that our chance of meeting in the wild most likely would have happened on campus, and that after that our chances declined.

# The Answer

```{r the_closest_distances, message = FALSE, warning = FALSE}

#Finding the dates that we were at 0m(+/- 111m) from each other
dates_crossed_paths <- whole_time_both %>%
  filter(distance == 0) %>%
  mutate(day = as.Date(timestamp))

#Visualising it
map_close_distances <- get_map(
  location = c(lon = median(
    c(
      dates_crossed_paths$longitude.x,
      dates_crossed_paths$longitude.y
    )
  ),
  lat = median(
    c(
      dates_crossed_paths$latitude.x,
      dates_crossed_paths$latitude.y
    )
  )),
  zoom = 17,
  scale = "auto",
  maptype = "toner-lite"
)

ggmap(map_close_distances) +
  geom_point(
    aes(x = longitude.y, y = latitude.y),
    data = dates_crossed_paths,
    colour = two_colours[2],
    size = 1,
    alpha = 0.5,
    position = "jitter"
  ) +
  geom_point(
    aes(x = longitude.x, y = latitude.x),
    data = dates_crossed_paths,
    colour = two_colours[1],
    size = 1,
    alpha = 0.5,
    position = "jitter"
  ) +
  theme_void() +
  labs(title = "Crossing paths on campus",
       subtitle = "Chan in blue, Dan in red. Each map represents one day where our paths crossed.") +
  facet_wrap( ~ day, ncol = 7, nrow = 5)

```

There were 35 *recorded* occasions where we may have crossed paths, mostly on campus. 35 times we may have been within arms reach of each other and we never even registered. It makes me feel very philosophical. In all likelihood we had walked past each other with our heads down, or in the clouds, paying very little notice to the people around us. Young Channon had not the foggiest clue what impact an unnoticed stranger would come to have on her. Since learning this I can't help but looked at the strangers in my day-to-day life in a different light.

### Our 35 almost-met-cute stories

I'm really have glad to have gone down this rabbit hole because now I think about the 35 near-misses we had, where if an audience had been watching they may have been on the edge of their seats. Like on the 16th of August 2017, exactly one week short of three years before we would meet, we're recorded at the same GPS coordinates in this spot where I frequently had lunch, near a coffee booth. Did he just trot past on the stairs? Did he take a seat in the shade under the trees? Did we queue together in line for coffee? I guess Google doesn't have the answer for everything. but it's pleasant to imagine.

![Crossing paths three years before we met, -33.959, 18.46.](images/Screenshot-2022-08-26-143714.png.png)

### Helpful resources

-   D. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2. The R Journal, 5(1), 144-161. URL <http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf>

-   Macarulla Rodriguez, Andrea & Tiberius, Christian & Bree, Roel & Geradts, Zeno. (2018). Google timeline accuracy assessment and error prediction. Forensic Sciences Research. 3. 240-255. 10.1080/20961790.2018.1509187.

-   movable-type.co.uk/scripts/latlong.html#<https://www.geeksforgeeks.org/program-distance-two-points-earth/#>:\~:text=For%20this%20divide%20the%20values,is%20the%20radius%20of%20Earth.
